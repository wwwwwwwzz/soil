"""
train_physics_constrained.py
è®­ç»ƒç‰©ç†çº¦æŸæ¨¡å‹ - å¹³è¡¡æƒé‡ç‰ˆæœ¬
"""

import torch
import numpy as np
from pathlib import Path
from torch.utils.data import DataLoader
from sklearn.metrics import r2_score, accuracy_score, classification_report
import sys
import time
from tqdm import tqdm

sys.path.append('..')

from models.physics_constrained_model import PhysicsConstrainedTransformer
from physics_loss import PhysicsConstrainedLoss
from interpretability_analysis import generate_interpretability_report
from experiments.train_improved import ImprovedHyperspectralDataset


def train_physics_model():
    # é…ç½® - å¹³è¡¡åˆ†ç±»å’Œå›å½’æƒé‡
    config = {
        'num_bands': 512,
        'd_model': 128,
        'n_heads': 4,
        'n_layers': 2,
        'num_classes': 3,
        'dropout': 0.3,
        'batch_size': 32,
        'learning_rate': 5e-4,
        'epochs': 150,
        'alpha': 0.45,  # æé«˜åˆ†ç±»æƒé‡ï¼ˆåŸ0.4ï¼‰
        'beta': 0.45,   # é™ä½å›å½’æƒé‡ï¼ˆåŸ0.5ï¼‰
        'gamma': 0.10   # ä¿æŒç‰©ç†çº¦æŸæƒé‡
    }

    print("ğŸš€ ç‰©ç†çº¦æŸæ¨¡å‹è®­ç»ƒå¼€å§‹ï¼ˆå¹³è¡¡æƒé‡ç‰ˆï¼‰")
    print("=" * 60)
    print("ğŸ“Š è®­ç»ƒé…ç½®:")
    print(f"  æŸå¤±æƒé‡è°ƒæ•´:")
    print(f"    åˆ†ç±»æƒé‡ Î±: 0.40 â†’ {config['alpha']} (â†‘)")
    print(f"    å›å½’æƒé‡ Î²: 0.50 â†’ {config['beta']} (â†“)")
    print(f"    ç‰©ç†çº¦æŸ Î³: {config['gamma']} (ä¸å˜)")
    print("-" * 60)
    for key, value in config.items():
        if key not in ['alpha', 'beta', 'gamma']:
            print(f"  {key:15s}: {value}")
    print("=" * 60)

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ”§ ä½¿ç”¨è®¾å¤‡: {device}")

    # æ•°æ®åŠ è½½
    data_path = Path('../data/processed')
    train_dataset = ImprovedHyperspectralDataset(data_path, mode='train')
    val_dataset = ImprovedHyperspectralDataset(data_path, mode='val')

    train_loader = DataLoader(train_dataset, batch_size=config['batch_size'],
                              shuffle=True, num_workers=0)
    val_loader = DataLoader(val_dataset, batch_size=config['batch_size'],
                            shuffle=False, num_workers=0)

    print(f"ğŸ“‚ æ•°æ®åŠ è½½å®Œæˆ:")
    print(f"  è®­ç»ƒæ ·æœ¬: {len(train_dataset):,}")
    print(f"  éªŒè¯æ ·æœ¬: {len(val_dataset):,}")

    # æ¨¡å‹
    model = PhysicsConstrainedTransformer(
        num_bands=config['num_bands'],
        d_model=config['d_model'],
        n_heads=config['n_heads'],
        n_layers=config['n_layers'],
        num_classes=config['num_classes'],
        dropout=config['dropout']
    ).to(device)

    # è®¡ç®—æ¨¡å‹å‚æ•°é‡
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)

    print(f"ğŸ§  æ¨¡å‹ä¿¡æ¯:")
    print(f"  æ€»å‚æ•°é‡: {total_params:,}")
    print(f"  å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
    print(f"  æ¨¡å‹å¤§å°: {total_params * 4 / 1024 / 1024:.2f} MB")

    # æŸå¤±å’Œä¼˜åŒ–å™¨ - ä½¿ç”¨æ–°çš„æƒé‡
    criterion = PhysicsConstrainedLoss(
        alpha=config['alpha'],
        beta=config['beta'],
        gamma=config['gamma']
    )

    optimizer = torch.optim.AdamW(model.parameters(), lr=config['learning_rate'])
    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(
        optimizer, T_0=10, T_mult=2
    )

    # è®­ç»ƒå†å²è®°å½•
    best_val_r2 = -float('inf')
    best_val_acc = 0
    best_epoch = 0
    history = {
        'train_loss': [], 'val_loss': [],
        'train_acc': [], 'val_acc': [],
        'train_r2': [], 'val_r2': [],
        'loss_breakdown': []
    }

    print(f"\nğŸƒâ€â™‚ï¸ å¼€å§‹è®­ç»ƒ ({config['epochs']} epochs)")
    print("=" * 80)

    # è®­ç»ƒå¾ªç¯
    for epoch in range(config['epochs']):
        start_time = time.time()

        # ========== è®­ç»ƒé˜¶æ®µ ==========
        model.train()
        train_losses = []
        train_accs = []
        train_preds = []
        train_labels = []
        train_regs = []
        train_reg_targets = []
        epoch_loss_breakdown = {
            'cls': [], 'reg': [], 'physics': [],
            'spectral': [], 'concentration': [], 'consistency': []
        }

        # è®­ç»ƒè¿›åº¦æ¡
        train_pbar = tqdm(train_loader, desc=f"Epoch {epoch+1:3d}/{config['epochs']} [Train]",
                         leave=False, ncols=100)

        for batch_idx, batch in enumerate(train_pbar):
            data, labels, norm_conc, orig_conc, _ = batch
            data = data.to(device)
            labels = labels.to(device)
            norm_conc = norm_conc.to(device)
            orig_conc = orig_conc.to(device)

            optimizer.zero_grad()
            outputs = model(data, return_attention=True)
            targets = (labels, norm_conc, orig_conc)

            loss, loss_dict = criterion(outputs, targets, model)
            loss.backward()

            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()

            # è®°å½•æŒ‡æ ‡
            train_losses.append(loss.item())
            class_preds = torch.argmax(outputs[0], dim=1)
            train_accs.append((class_preds == labels).float().mean().item())

            train_preds.extend(outputs[1].squeeze().detach().cpu().numpy())
            train_labels.extend(labels.cpu().numpy())
            train_regs.extend(outputs[1].squeeze().detach().cpu().numpy())
            train_reg_targets.extend(norm_conc.cpu().numpy())

            # è®°å½•æŸå¤±åˆ†è§£
            for key in epoch_loss_breakdown:
                if f'loss_{key}' in loss_dict:
                    epoch_loss_breakdown[key].append(loss_dict[f'loss_{key}'])

            # æ›´æ–°è¿›åº¦æ¡
            train_pbar.set_postfix({
                'Loss': f'{loss.item():.4f}',
                'Acc': f'{train_accs[-1]:.3f}',
                'LR': f'{optimizer.param_groups[0]["lr"]:.2e}'
            })

        # ========== éªŒè¯é˜¶æ®µ ==========
        model.eval()
        val_losses = []
        val_accs = []
        val_preds_class = []
        val_preds_reg = []
        val_labels = []
        val_reg_targets = []

        val_pbar = tqdm(val_loader, desc=f"Epoch {epoch+1:3d}/{config['epochs']} [Val]  ",
                       leave=False, ncols=100)

        with torch.no_grad():
            for batch in val_pbar:
                data, labels, norm_conc, orig_conc, _ = batch
                data = data.to(device)
                labels_dev = labels.to(device)
                norm_conc_dev = norm_conc.to(device)
                orig_conc_dev = orig_conc.to(device)

                outputs = model(data, return_attention=True)
                targets = (labels_dev, norm_conc_dev, orig_conc_dev)

                loss, _ = criterion(outputs, targets, model)

                val_losses.append(loss.item())
                class_preds = torch.argmax(outputs[0], dim=1)
                val_accs.append((class_preds == labels_dev).float().mean().item())

                val_preds_class.extend(class_preds.cpu().numpy())
                val_preds_reg.extend(outputs[1].squeeze().cpu().numpy())
                val_labels.extend(labels.numpy())
                val_reg_targets.extend(norm_conc.numpy())

                val_pbar.set_postfix({
                    'Loss': f'{loss.item():.4f}',
                    'Acc': f'{val_accs[-1]:.3f}'
                })

        # è®¡ç®—epochæŒ‡æ ‡
        train_loss_avg = np.mean(train_losses)
        val_loss_avg = np.mean(val_losses)
        train_acc_avg = np.mean(train_accs)
        val_acc_avg = accuracy_score(val_labels, val_preds_class)

        # è®¡ç®—RÂ²
        train_r2 = r2_score(train_reg_targets, train_regs) if len(train_regs) > 1 else 0
        val_r2 = r2_score(val_reg_targets, val_preds_reg) if len(val_preds_reg) > 1 else 0

        # è®°å½•å†å²
        history['train_loss'].append(train_loss_avg)
        history['val_loss'].append(val_loss_avg)
        history['train_acc'].append(train_acc_avg)
        history['val_acc'].append(val_acc_avg)
        history['train_r2'].append(train_r2)
        history['val_r2'].append(val_r2)

        # è®¡ç®—å¹³å‡æŸå¤±åˆ†è§£
        avg_loss_breakdown = {k: np.mean(v) if v else 0 for k, v in epoch_loss_breakdown.items()}
        history['loss_breakdown'].append(avg_loss_breakdown)

        # å­¦ä¹ ç‡è°ƒæ•´
        scheduler.step()

        # è®¡ç®—è®­ç»ƒæ—¶é—´
        epoch_time = time.time() - start_time

        # æ‰“å°è¯¦ç»†ä¿¡æ¯
        if (epoch + 1) % 5 == 0 or epoch == 0:
            print(f"\nğŸ“Š Epoch {epoch+1:3d}/{config['epochs']} - {epoch_time:.1f}s")
            print(f"  ğŸ‹ï¸ è®­ç»ƒ | Loss: {train_loss_avg:.4f} | Acc: {train_acc_avg:.3f} | RÂ²: {train_r2:.4f}")
            print(f"  ğŸ¯ éªŒè¯ | Loss: {val_loss_avg:.4f} | Acc: {val_acc_avg:.3f} | RÂ²: {val_r2:.4f}")

            # æ˜¾ç¤ºç‰©ç†æŸå¤±åˆ†è§£
            if avg_loss_breakdown:
                print(f"  ğŸ”¬ æŸå¤±åˆ†è§£:")
                print(f"     åˆ†ç±»: {avg_loss_breakdown.get('cls', 0):.4f} ({config['alpha']*100:.0f}%æƒé‡)")
                print(f"     å›å½’: {avg_loss_breakdown.get('reg', 0):.4f} ({config['beta']*100:.0f}%æƒé‡)")
                print(f"     ç‰©ç†: {avg_loss_breakdown.get('physics', 0):.4f} ({config['gamma']*100:.0f}%æƒé‡)")

            print(f"  ğŸ“ˆ å­¦ä¹ ç‡: {optimizer.param_groups[0]['lr']:.2e}")

            # ä¿å­˜æœ€ä½³æ¨¡å‹ - ç»¼åˆè€ƒè™‘å‡†ç¡®ç‡å’ŒRÂ²
            # ä½¿ç”¨åŠ æƒè¯„åˆ†ï¼š0.5 * acc + 0.5 * r2
            val_score = 0.5 * val_acc_avg + 0.5 * val_r2
            best_score = 0.5 * best_val_acc + 0.5 * best_val_r2

            if val_score > best_score:
                best_val_r2 = val_r2
                best_val_acc = val_acc_avg
                best_epoch = epoch
                torch.save({
                    'epoch': epoch,
                    'model_state_dict': model.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                    'val_r2': val_r2,
                    'val_acc': val_acc_avg,
                    'config': config,
                    'history': history
                }, 'best_physics_model_balanced.pth')
                print(f"  âœ… æ–°çš„æœ€ä½³æ¨¡å‹å·²ä¿å­˜ (ç»¼åˆè¯„åˆ†: {val_score:.4f})")

            print("-" * 80)

    # è®­ç»ƒå®Œæˆ
    print(f"\nğŸ‰ è®­ç»ƒå®Œæˆ!")
    print(f"ğŸ“ˆ æœ€ä½³éªŒè¯ç»“æœ (Epoch {best_epoch+1}):")
    print(f"   å‡†ç¡®ç‡: {best_val_acc:.4f}")
    print(f"   RÂ²: {best_val_r2:.4f}")
    print(f"   ç»¼åˆè¯„åˆ†: {0.5 * best_val_acc + 0.5 * best_val_r2:.4f}")

    # åŠ è½½æœ€ä½³æ¨¡å‹è¿›è¡Œæœ€ç»ˆè¯„ä¼°
    checkpoint = torch.load('best_physics_model_balanced.pth', weights_only=False)  # æ·»åŠ weights_only=False
    model.load_state_dict(checkpoint['model_state_dict'])
    model.eval()

    # æœ€ç»ˆéªŒè¯é›†è¯„ä¼°
    print(f"\nğŸ” æœ€ç»ˆæ¨¡å‹è¯„ä¼°:")
    final_preds_class = []
    final_preds_reg = []
    final_labels = []
    final_reg_targets = []

    with torch.no_grad():
        for batch in tqdm(val_loader, desc="Final Evaluation"):
            data, labels, norm_conc, orig_conc, _ = batch
            data = data.to(device)
            labels_dev = labels.to(device)
            norm_conc_dev = norm_conc.to(device)

            outputs = model(data, return_attention=True)

            class_preds = torch.argmax(outputs[0], dim=1)
            final_preds_class.extend(class_preds.cpu().numpy())
            final_labels.extend(labels.numpy())
            final_preds_reg.extend(outputs[1].squeeze().cpu().numpy())
            final_reg_targets.extend(norm_conc.numpy())

    final_acc = accuracy_score(final_labels, final_preds_class)
    final_r2 = r2_score(final_reg_targets, final_preds_reg)

    print(f"  åˆ†ç±»å‡†ç¡®ç‡: {final_acc:.4f}")
    print(f"  å›å½’RÂ²:    {final_r2:.4f}")
    print(f"  ç»¼åˆè¯„åˆ†:   {0.5 * final_acc + 0.5 * final_r2:.4f}")

    # åˆ†ç±»æŠ¥å‘Š
    print(f"\nğŸ“‹ è¯¦ç»†åˆ†ç±»æŠ¥å‘Š:")
    class_names = ['æ¸…æ´ (0)', 'è½»åº¦æ±¡æŸ“ (3)', 'é‡åº¦æ±¡æŸ“ (4)']
    print(classification_report(final_labels, final_preds_class,
                              target_names=class_names, digits=4))

    # ä¸åŸºçº¿å¯¹æ¯”
    print("\nğŸ“Š ä¸åŸºçº¿æ¨¡å‹å¯¹æ¯”:")
    baseline_results = {
        'Standard Transformer': {'acc': 0.9688, 'r2': 0.9337},
        'SVM': {'acc': 0.9628, 'r2': 0.8652},
        'PLSR': {'acc': 0.9400, 'r2': 0.8517}
    }

    for model_name, results in baseline_results.items():
        print(f"  {model_name:20s}: Acc={results['acc']:.4f}, RÂ²={results['r2']:.4f}")
    print(f"  {'Physics-Constrained':20s}: Acc={final_acc:.4f}, RÂ²={final_r2:.4f} â† æ–°æƒé‡")

    # è®¡ç®—æå‡
    print("\nğŸ¯ æ€§èƒ½æå‡åˆ†æ:")
    st_acc_improve = (final_acc - 0.9688) / 0.9688 * 100
    st_r2_improve = (final_r2 - 0.9337) / 0.9337 * 100
    print(f"  ç›¸æ¯”Standard Transformer:")
    print(f"    å‡†ç¡®ç‡å˜åŒ–: {st_acc_improve:+.2f}%")
    print(f"    RÂ²å˜åŒ–:    {st_r2_improve:+.2f}%")

    if final_acc >= 0.96 and final_r2 >= 0.94:
        print("\nâœ¨ ä¼˜ç§€ï¼æ¨¡å‹åœ¨ä¿æŒRÂ²ä¼˜åŠ¿çš„åŒæ—¶è¾¾åˆ°äº†é«˜å‡†ç¡®ç‡ï¼")
        print("ğŸ“ è¿™ä¸ªç»“æœå®Œå…¨ç¬¦åˆSCIè®ºæ–‡å‘è¡¨è¦æ±‚")

    # ç”Ÿæˆç‰©ç†å¯è§£é‡Šæ€§æŠ¥å‘Š
    print(f"\nğŸ”¬ ç”Ÿæˆç‰©ç†å¯è§£é‡Šæ€§æŠ¥å‘Š...")
    results_dir = Path('../results')
    results_dir.mkdir(exist_ok=True)

    try:
        scores = generate_interpretability_report(model, val_loader, save_dir='../results/')
        print(f"ğŸ“Š ç‰©ç†ä¸€è‡´æ€§æ€»åˆ†: {scores.get('Overall', 0):.3f}")
        print(f"ğŸ“ æŠ¥å‘Šå·²ä¿å­˜åˆ°: {results_dir.absolute()}")
    except Exception as e:
        print(f"âš ï¸ å¯è§£é‡Šæ€§åˆ†æå‡ºé”™: {e}")
        scores = None

    return model, scores


if __name__ == "__main__":
    model, scores = train_physics_model()